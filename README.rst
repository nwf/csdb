What is this?
#############

Resurrecting an ancient project, a "mostly read-only file management tool".
It's intended for keeping a large list of checksums in a database so that
duplication, movement, and corruption of files can be detected.  In addition to
maintaining a singular database, it also offers cross-database functionality.

We speak of "observations" to mean an association of a file path and its
contents (or, at least, their cryptographic checksum).  Most operations on the
checksum database pertain to one or more observations.

Theory of Operation
###################

This program is just a shim around a database; it does not interact with the
filesystem much itself.  Instead, it should be used in composition with things
like ``find`` and the GNU coreutils digest programs (e.g. ``sha512sum``),
delegating details of filesystem traversal and choice of hash and so on to the
user.

Supported Operations
####################

Initialize A Database
=====================

::

   cdb --db ${DB} init

Observe A Path
==============

Add the checksum of a single path to the database.  This will create a new
checksum and/or a new path identifier as needed and will bind them together. ::

   sha512sum $FILE | cdb --db ${DB} addh

Or, for all files under a path::

   find $DIR -type f -exec sha512sum {} \+ | cdb --db ${DB} addh

If we have a pile of digest files already, each of which contains digests of
paths relative to its location, we can generate a database, ``${DB2}`` from them
with the assistance of the ``digestrelative`` tool::

  find ${DIR} -type f -name SHA512SUMS -print0 | cdb-digestrelative --inul | cdb --db ${DB} addh

Revalidate A Path Observation
=============================

Measure the checksum of a path and confirm that the database already held that
observation.  Reports unexpected files as well as mis-checksummed contents. ::

   sha512sum $FILE | cdb --db ${DB} verh

Or for all files under a path::

   find $DIR -type f -exec sha512sum {} \; | cdb --db ${DB} verh

This processing of digest streams is to be preferred to verifying a digest
stream as generated by the database, e.g.::

   cdb --db ${DB} look \* | sha512sum -c

because the former can be more informative in the case of mismatching digests
(specifically, the database can look for other paths that have the reported
digest).  If it's easier to have the database generate the set of files, that
can be done::

   cdb --db ${DB} look \* --no-hashes --unescape --nul | xargs -0 sha512sum | cdb --db ${DB} verh

Add Checksums For Missing Files
===============================

We can quickly construct a "just paths" database, which associates all paths
with a single digest, from the current state of the file system as follows::

   cdb --db ${JPDB} init
   find ${DIR} -type f -printf "0  %p\\0" | ./cdb --db ${JPDB} addh --inul

This database may not seem very useful, but when combined with ``cdb diff`` we
can quickly find all paths whose checksums are unknown to the database::

   cdb --db ${DB} diff ${JPDB} --flavor=path --which=super

We can then script computing those files' checksums and adding the new reports
to the database::

   cdb --db ${DB} diff ${JPDB} --flavor=path --which=super --no-headers --nul --unescape > ${JPDB}.new-files0
   xargs -0 sha512sum > ${JPDB}.new < ${JPDB}.new-files0
   cdb --db ${DB} addh < ${JPDB}.new

From Another Database
---------------------

If we have another database that knows digests for our files, rather than
computing digests again, we can extract checksums from ``${DB2}`` and install
them into ``${DB}``::

   cdb --db ${DB2} mapp --inul < ${JPDB}.new-files0 | cdb --db ${DB} addh

Responding to File Moves
========================

Armed with a "just paths" database as per the above, we can then direct the
database to prune tracked paths not in the "just paths" database if the hashes
are observed elsewhere::

   cdb --db ${DB} diff ${JPDB} --flavor=path --which=sub --no-headers --nul --unescape > ${JPDB}.missing-files0
   cdb --db ${DB} domv --inul < ${JPDB}.missing-files0
   cdb --db ${DB} gc > ${DB}.gc
   sqlite3 ${DB} < ${DB}.gc

.. TODO or if the observed digest is now superseded?

Find Duplicates
===============

Given a path prefix (possibly empty), report all logged observations below that
path of contents that exist in multiple locations (i.e., files with checksum
collisions).

.. TODO

Remove Path
===========

Cease to consider a particular path part of the database and remove all
observations made of it.  Since this application is primarily for data hoarders
who tend not to delete things, one should prefer to :ref:`Respond to File Moves
<Responding to File Moves>` rather than risk removing the last observation of a
given hash.

.. TODO

Add Superseder
==============

Indicate that some file contents are to be considered a lesser version of some
other contents.  

.. TODO

Report Novelty
==============

Given a path, measure its checksum and report if it does not match, and has not
been superseded by, any observation already recorded in the database.

.. TODO

.. This command would be useful for ingesting things into a library or pruning
   collections of files outside the library.

--------------------------------------------------------------------------------

Example Uses
############

A photo library
===============

Suppose ``/mnt/photos`` contains a collection of photos.  We might want to...

* measure all the files in that directory, flagging new and updated contents::

    $ cksdb /mnt/photos/.cksdb observe /mnt/photos

* measure all the files in that directory, automatically updating the database::

    $ cksdb /mnt/photos/.cksdb observe --new --changed /mnt/photos

* report duplicates anywhere in the library::

    $ cksdb /mnt/photos/.cksdb ls --duplicate

* report files in a particular directory that also exist anywhere else in the
  library::

    $ cksdb /mnt/photos/.cksdb ls --duplicate /mnt/photos/dir1

* restrict the search for duplication to another direcotry::

    $ cksdb /mnt/photos/.cksdb ls --duplicate /mnt/photos/dir1 \
      --also /mnt/photos/dir2

* explicitly acknowledge a deletion by removing observations of it::

    $ cksdb /mnt/photos/.cksdb rm /mnt/photos/filename

* indicate that the last observed content of ``foo.jpg`` is superseded by the
  last observed content of ``foo.raw``::

    $ cksdb /mnt/photos/.cksdb supersede /mnt/photos/foo.jpg /mnt/photos/foo.raw

* import files from outside the library, say, in ``/mnt/sdcard``, skipping
  duplicate and superseded files and removing all examined files (that is,
  imported, duplicate, and superseded; ``--harvest``)::

    $ cksdb /mnt/photos/.cksdb import --harvest /mnt/photos/newdir /mnt/sdcard

* import from another database::

    $ cksdb /mnt/photos/.cksdb import-db /mnt/oldphotos/.cksdb

Cross-Database Operations
=========================

Compute violations of set-theoretic relationships between a database and the
union of one or more other databases::

    $ cksdb /mnt/photos/.cksdb is-subset /mnt/backups/photos/.cksdb
    $ cksdb /mnt/photos/.cksdb is-superset /mnt/backups/photos/.cksdb
